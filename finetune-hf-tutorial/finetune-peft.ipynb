{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4882833-8a07-4d59-a904-9a6b32d6e840",
   "metadata": {},
   "source": [
    "# PEFTのアダプタを読み込む"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc10fa-9d7c-4019-83e7-1a1bbfcba9af",
   "metadata": {},
   "source": [
    "Based on <https://huggingface.co/docs/transformers/ja/peft>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f6d10-f0be-40c1-990c-5aaab886c710",
   "metadata": {},
   "source": [
    "PEFTアダプターを読み込む方法は2つある。\n",
    "\n",
    "1. adapter_config.jsonのあるモデルをロードする。モデルとアダプターがセットで読み込まれる\n",
    "2. モデルを読み込んでから、アダプタを追加で読み込む\n",
    "\n",
    "読み込み時に量子化できる。学習用に新規のアダプタを作成できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f06698-2aa1-423e-9fc3-3b8f56ad1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルとアダプタをセットで読み込む例\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4425fc4-67ac-4852-92bb-250813279f9d",
   "metadata": {},
   "source": [
    "ここで読み込んでいるアダプター: <https://huggingface.co/ybelkada/opt-350m-lora/tree/main>\n",
    "\n",
    "<https://huggingface.co/facebook/opt-350m> に対するアダプタだが、何を学習済みなのかは不明。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11254ad8-8c84-446e-9992-4f8c8ea22e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを読み込んでから、アダプタを追加で読み込む\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d13fff-9188-4afd-929d-05e1252ccdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルとアダプタをセットで、量子化して読み込む(=QloRA)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09056e-3095-4bfe-8d78-fb748b31e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# アダプタを新規に作成してアタッチする\n",
    "\n",
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    init_lora_weights=False\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088eb3b1-0984-41c7-bed6-0b9756467e2d",
   "metadata": {},
   "source": [
    "以下はアダプタを複数作って使用時に選択する例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c30a07-af8d-400a-be8b-06dc15624ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach new adapter with same config\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a0467-2bd6-47ae-b135-38da9511a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use adapter_1\n",
    "# model.set_adapter(\"adapter_1\")\n",
    "# output = model.generate(**inputs)\n",
    "# print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))\n",
    "\n",
    "## use adapter_2\n",
    "# model.set_adapter(\"adapter_2\")\n",
    "# output_enabled = model.generate(**inputs)\n",
    "# print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91637e2d-f8db-483d-9bee-56e2ad127ca9",
   "metadata": {},
   "source": [
    "## PEFTアダプターを学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ca932-984d-4795-ab9c-35e77bb1e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b7262-26aa-4eef-9e94-a0d9acc1d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047ae4f-8be9-408b-a23e-bc4e294d9129",
   "metadata": {},
   "source": [
    "あとはTrainerで学習するだけ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fd16f-99e4-4678-b256-8b70c798a661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, ...)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c2aa6-39f3-4ef2-a117-fc2e470120be",
   "metadata": {},
   "source": [
    "こうなると LoraConfigの内容を理解するほうが先決そう。\n",
    "\n",
    "上の例では中間層の数 r=64 、biasなし、CAUSAL_LMに対するアダプテーションだということはわかる。\n",
    "alphaとdropoutがわかってない。\n",
    "\n",
    "[ybelkada/opt-350m-lora](https://huggingface.co/ybelkada/opt-350m-lora/blob/main/adapter_config.json) の alpha と dropout は 32 と 0.05 で r=16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0713b-90b4-4384-b589-584b25e93c32",
   "metadata": {},
   "source": [
    "[LoraConfigのドキュメント](https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig)\n",
    "\n",
    "全部で19個のパラメータ、以下は抜粋:\n",
    "\n",
    "* r - attention の時限\n",
    "* alpha - LoRA scaling ?\n",
    "* dropout - LoRA層のドロップアウト確率 ?\n",
    "* bias - バイアスのタイプ: none, all, lora_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e4c4a-0c20-484f-9418-3943fef752f6",
   "metadata": {},
   "source": [
    "[C3TR-Adapterのconfig](https://huggingface.co/webbigdata/C3TR-Adapter/blob/main/adapter_config.json)\n",
    "\n",
    "* r=128\n",
    "* alpha=64\n",
    "* dropout=0\n",
    "* target_modules: gate_proj, v_proj, k_proj, o_proj, up_proj, q_proj, down_proj\n",
    "\n",
    "gemma2に対してたったr=128で翻訳できるようになってるの、ちょっと驚き"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
